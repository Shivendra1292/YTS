import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
stopwords = list(STOP_WORDS)

document1 ="""hey everyone welcome back to my channel the new video I'm making this video because uh in the past uh I have created some webd tutorials and I recently launched my uh Genna uh and introduction to rag course which is going very well a few thousand signups already so make sure you you know check that out and the question I'm getting quite a lot is Kunal when we create our you know websites and projects when you do research work and like you universities or or you know other areas and uh when you're training your machine learning models and all these things that I've been doing a common question in all of these domains that I get is where do you get the data from right so there are some publicly available data online some people you know script the websites and get data from that and some people use like uh temporary data using psyit learn or whatever but uh the main question that I have gotten is Kunal when you're trying to get some real world data that is publicly available even though it's publicly available it's not easy and I think data is a very important topic to speak about both in terms of like you know data security the ethical considerations of how to use data how to manage large scale data and then all the tools revolving around that so this video is going to be beneficial for you you know if you're looking to find uh data to train your machine learning models for your own research work for your projects and all sorts of things because I'm doing a machine learning course uh like a mini course and I'll be doing a main machine learning course uh later on as well this video is going to be highly relevant U so we're going to talk about how do we scrape data from the websites and publicly available information in an ethical way before we get started with that I want to share a bit around my experiences with um you know the problems that I have when trying to scrape data from the internet and let me know in the comment section below if you suffer from the same challenges so in my experience with data scraping I've encountered like several challenges that can make the collection process quite tricky and you know technical hurdles legal restrictions and also ethical considerations all play a role in shaping the landscape of data extraction depends you know where you're extracting the data from one of the most noticeable and notable challenges that I have faced is dealing with B detection mechanisms on the websites now if you don't know what that is you know these systems are set up to spot automated scraping activities and can be quite sophisticated so they analyze things like user agent strings and request rates to differentiate between a human user and Bots which can sometimes you know put a road block uh road block in my um in my scraping efforts right other than that uh you know another obstacle that I have come across is uh solving captas during the scraping process and these challenges are specifically designed uh you know to readed out Bots by presenting tasks that are easy for humans but difficult for machines you may have seen like captas right and uh you might be saying there are many solutions out there you know who bypass captas but they often come with their own set of complications and it basically includes um you know like delays and increased complexity which is something I don't really like and if that's not enough on top of that you know there are also legal and ethical considerations to navigate um so you know scraping data without proper authorization can land in some pretty you know in a hot water potentially violating terms of service agreements and like copyright laws and all sorts of things plus scraping large amounts of data can also strain a website's um servers and disrupts its performance so it can raise like some ethical concerns about the impact on the other people as well and uh yeah there's just you know quite a lot of uh challenges um when we talk about scraping data from the internet so let's talk a little bit more about what's the right way to do it I've been using bright data and I shared it in my previous videos as well people loved it by the way if you haven't already seen that check it out but in this video we're solving these specific challenges so let's see what it's all about so we're going to be taking a look at the web unlocker tool by by bright data if you check the website uh you can see that it provides capture solving IP rotations and re-entries JavaScript rendering uh 99.9% success rate pay only for Success which is pretty cool and by the way if you use the link in the description to sign up you'll get some additional credits or uh something like that like $15 I believe but you can check out the links in the description below so it provides website unlocking so AI powered website unlocking access to public web pages effortlessly with the advanced web un Locker platform and all the blockers that I mentioned like capture solving and uh browser fingerprinting iation so on and so forth takes care of that there proxy management as as well so no more burning through IPS um and it takes care takes care of it for you um there's also a buil-in browser for JavaScript rendering because certain websites use JavaScript uh uh that require the launch of a browser in order to fully display certain elements on the data so web un Locker can help with that and uh you know it's so it's pretty awesome you can try it for free and you can use the link in the description to get some credits as well we're obviously going to be seeing a demo and uh you know we're going to be making a request we're going to be seeing how that works and get our data and it's going to be work for working for like we're going to try for various websites and um yeah the whole idea is you know for you to not get blocked and um an instance for this can be if you're let's say trying to automate Instagram for example um it can block you pretty easily right if Instagram sees like okay there's a bot your your account would be blocked so in order to not get blocked and um all these other things we're going to be using bright data so all these challenges as you can see you know uh being solved by bright data and uh we're going to be taking a look at demo right now but you can check more about it uh on the website you can see some nice partners and people who use it already and um this video is going to be set as a foundation for you to you know collect data for for all your work but also the courses and stuff that I do we're going to be utilizing it quite a lot to get the data and uh do dat do our day-to-day you know data related data scraping related tasks so I'm I'm sure you are getting blocked with one of the reasons that I just mentioned highly recommend trying trying out uh bright data and let's see how we can get started right now all right I am going to log in and you can start a free trial I'm going to log in we make devs not a problem and that should be good now I'm going to go to my I'm going to go to proxies and here you can see the web un Locker so you can just click on get started now here what we have is that um we're going to put a name in it so I can put any name that I want um Kunal demo or I can just put demo now geolocation targeting so obviously I already mentioned the benefits you can bypass captas and blocks and all the restrictions only pay for successful requests uh scrap public data from any website automated IP rotation and simulation and fingerprints geolocation targeting we're going to select uh country over here so you can see on the screen and uh do note that the zones the the name cannot be changed later on you can select premius uh premium domains if you want you can select um a requests uh some Advanced C custom headers and cookies when enabled you can send custom headers and cookies along with your requests okay interesting geolocation targeting based on um in this Zone the location is selected per request so I'm going to do it country wise you will also be able to Target location based on the resolutions you specify Here country wise is fine I'm going to click on ADD you are about to create a new Zone demo are you sure I am sure that should be good access parameters you ready to send your first web unlocker request you can copy paste this command in your terminal to send your first web unlocker request in guides and different Pro check out the code Integrations as well if you want to do that you can review our fuel documentation we here okay coding integration examples can also be found over here so if I'm going to do an API for let's say python there you go pretty simple and here you can find the documentation as well Okay cool so getting started or a basic API and you can have a destination site so if you want to scrap um whatever you want to scrape you can put it over here you can add in your proxy you can add in a country for let's say if I add United States I'm going to hide the advanced options then you just select your language and here you go you have your language the whole um code written for you and try Java as well there you go pretty cool okay so that was the setting up part okay now let me try to scrape the good reads and here you can see here's my host uh my username my password you can add in more passwords allow IPS you can block some IPS Target host can configure it as well we'll check out the premium domains later on because there are some websites uh which are harder to scrape for example these ones for that you can enable premium domains uh in synchronous request we already mentioned that you can add in custom headers and cookies but uh let me just copy this paste it and let me just copy this URL you can also obviously integrate it in your um you know in your um code sample as well and that should give me codes Jack Browns when I begin in it see here we go so there was one blocker over here which was if I go to this URL you can't see it right now but if I do it in uh Incognito you will if I go to goodreads.com go to best quotes it yeah gives me like a popup or something okay so in order for me to not get blocked I'm using the the web the biodata platform other than that I want to show you um here I have it's all good um can check out the statistics total you know based on your usage um there's a list of websites like academy.com by go to academy.com and we try to scrape this it's going to be a bit bit tricky let me try that okay um academy.com requires a premium permission please enable the premium permission and then do it I can try that premium domains save Zone demo plan will be changed yep not a problem configuration saved let's try it again so now the hardware to scrap websites I talk about academy.com don't allow these usually have like captas and uh you know a lot of other things let try it now should work okay there there you go see cool cool this a bit dirty right now but you can obviously parse it and scrape it and uh you know parse it and make it pretty and get some specific data out of it but uh that's it yeah pretty cool so you're able to scale like uh your you're able to scrape challenging websites as well uh now that you want carousel.com so you can see there's this little capture that just the the verification that just happened and here's a I don't know if you can see it going to move my screen here there you can see a little symbol over there um but I'm going to try and going to try and scrape this now there you go that should work easily there we go cool so you have that HTML page that you've just scraped the entire thing but it doesn't have to be like um you don't have to it I know it looks a bit dirty right now but uh yeah should be should be okay when you par it all right uh I went on Reddit SL webscraping and it has a question the most difficult websites to scrape someone said Walmart which has some nice upwards so what I'm going to do is I'm going to try and scrape um I'm going to try and scrape Walmart as well and um you know if sometimes you may find websites that are blocked behind a signin account then that that also you can work with the bright data in order to do that um I've done a separate video on such things as well so make sure you check it out uh shop all groceries um going to run this and see how it goes okay so I'm going to run this and see how how this goes there you go here we have it nice see vegetarian food seafood frozen meat very cool very cool all right now I know what you're saying Kunal it looks a bit ugly but uh you can also you know par this data like I do simply can do let's say python I have Walmart food section over here you States basic API demo and this is what the code that I just copied looks like I'm going to add in uh beautiful soup over here just to pass this uh this data and here I've just added it in the this entire thing is same what bright data has given me I have just made this change and let's say I'm trying to display all the specific data for example in this case all the paragraphs in this request so I just go to a new terminal uh and just run this file and it will give me in a prettier way all the paragraphs there you go stick chicken all the paragraphs I have gotten over here okay so let me show you a nice real example um I am going to do another python project um this is fine I'm going to scrape this website for a list of companies here in India uh any country is okay and I will just copy this copy this code and here I'm just going to paste my code not a problem all right I'm going to remove this um need the extra stuff and don't need this as well it's fine let's keep it simple now what we want to do is we just want to get all the data for the companies okay all the company names let's say for my machine learning model I I need all the company names um for instance could be anything here I'm also going to import uh beautiful soup don't need this here performing request is fine but I'm going to save this thing actually in it is response so I'm going to say response is equal to this okay now soup is equal to beautiful soup response and HTML passer okay now we can just pass it I can say for I in soup dot find all let's say you want the company so I can say just heading to okay and here I can just say print I do text Dot I can just strip the extra you know uh spaces that should be it terminal new terminal there you go give me the name of the companies nice awesome cool TCS H Vio HDFC Bank Tech Mahindra exess bank so this is just an example now it depends on the type of data you know when you talk about machine learning um you can scrape Amazon you can get the prices of uh various fruit items or whatever based on you can also get the ears and everything and uh as simple as that how easy was that cool nice um but yeah pretty simple stuff you someone commented that Walmart was the really difficult one but not with bright data it's very cool and uh last thing I want to tell you is that uh I highly recommend checking out the documentation so just go to you know um go to the bright data document which should be if you just go to Bright data documentation web un Locker here you can find the documentation okay and a lot more other things so I highly recommend checking it out and uh I've done a video on this already but there are also pre-built Uh custom data sets for you based on various categories that you can check out and custom data sets that you can create and uh proxies residential proxies if you're looking to try those out so it's an allinone solution that I highly recommend you know for you to try out and um use for your data scraping needs but yeah most difficult websites pretty easy to scrape and we'll be utilizing it quite a lot in the in the future in our boot camp and courses as well but yeah if you're making a project you need data publicly available data for your for your projects for your you know coursework or for just you know anything um check out bar data and if you use the link in the description below you'll get get some nice free credits as well to test it out and uh that's basically about it if you have any further questions let me know in the comment section below and uh also um if you have any suggestions uh like you want me to create a specific video about a specific use case or a website you want me to scrape let me know in the comment section below and uh I'll see you in the next one and um yeah go scrip data uh for training your machine learning models I'll be doing it in my boot camps as well yeah see you in the next one have a great [Music] day"""
nlp = spacy.load('en_core_web_sm')
docx = nlp(document1)
mytokens = [token.text for token in docx]

# Build Word Frequency
# word.text is tokenization in spacy
word_frequencies = {}
for word in docx:
    if word.text not in stopwords:
            if word.text not in word_frequencies.keys():
                word_frequencies[word.text] = 1
            else:
                word_frequencies[word.text] += 1
print(word_frequencies)


maximum_frequency = max(word_frequencies.values())
for word in word_frequencies.keys():  
        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

sentence_list = [ sentence for sentence in docx.sents ]

# Sentence Score via comparrng each word with sentence
sentence_scores = {}  
for sent in sentence_list:  
        for word in sent:
            if word.text.lower() in word_frequencies.keys():
                if len(sent.text.split(' ')) < 300:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word.text.lower()]
                    else:
                        sentence_scores[sent] += word_frequencies[word.text.lower()]

from heapq import nlargest

summarized_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)
print(summarized_sentences)
print(len(summarized_sentences))
final_sentences = [ w.text for w in summarized_sentences ]
summary = ' '.join(final_sentences)
print(len(summary))
print(len(document1))